from pathlib import Path
import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import pandas as pd
from scipy.spatial.distance import pdist, squareform
from scipy.spatial import Delaunay
import networkx as nx
import torch
from torch_geometric.nn import Node2Vec
from torch_geometric.utils import from_networkx
import os
import torch


os.environ['TORCH'] = torch.__version__
print(torch.__version__)


get_ipython().getoutput("conda info --envs")


##select dir
import os
os.getcwd()
data_dir = Path("/ibex/scratch/medinils/breast_data/data/")


##load the data
train_features_dir = data_dir / "train_input" / "moco_features"
test_features_dir = data_dir / "test_input" / "moco_features"
df_train = pd.read_csv(data_dir/"supplementary_data"/"train_metadata.csv")
df_test = pd.read_csv(data_dir/"supplementary_data/test_metadata.csv")

# concatenate y_train and df_train
y_train = pd.read_csv(data_dir  / "train_output.csv")
df_train = df_train.merge(y_train, on="Sample ID")

print(f"Training data dimensions: {df_train.shape}")  # (344, 4)
df_train.head()


#create a graph dic to store them
graphs = {}

X_train = []
y_train = []
centers_train = []
patients_train = []

for sample, patient, center, label in tqdm(
    df_train[["Sample ID", "Patient ID", "Center ID", "Target"]].values
):
    # coordinates and features each iteration one sample
    _features = np.load(train_features_dir / sample)
    # coo and features, separate them
    coo, features = _features[:, 1:3], _features[:, 3:]
    # delaunay triangulation
    tri = Delaunay(coo)
    # Create the graph to add the
    G = nx.Graph()

    # add the edges for each Delaunay triangulation
    for tri in tri.simplices:
        for i in range(3):
            for j in range(i+1, 3):
                # add the edge between the i-th and j-th point in the triangle
                # the edge weight is the euclidean distance
                p1 = coo[tri[i]]
                p2 = coo[tri[j]]
                distance = np.linalg.norm(p1 - p2)
                G.add_edge(tri[i], tri[j], weight =distance)
                # add features as node attributes
                G.nodes[tri[i]].update({'features': features[tri[i]]})
                G.nodes[tri[j]].update({'features': features[tri[j]]})
    # store the graphs for each patient (key)
    graphs[patient] = G
    
    # Slide-level averaging
    X_train.append(np.mean(features, axis=0))
    y_train.append(label)
    centers_train.append(center)
    patients_train.append(patient)


X_train = np.vstack(X_train)
print(X_train.shape)


y_train = np.array(y_train)
print(y_train.shape)


## check the first graph
G1 = next(iter(graphs.values()))
print("Number of nodes (tiles)", len(G1.nodes))

# Calculate the layout with more spacing
pos = nx.spring_layout(G1, k=0.10)
nx.draw_networkx(G1, pos, node_size=10, width= 0.3,with_labels=False)
plt.show()

features = G1.nodes[1]["features"]



import pickle

#save it
save_path = "/ibex/scratch/medinils/breast_data/data/process/graphs.pkl"
with open(save_path, "wb") as f:
    pickle.dump(graphs, f)


# load it
# Load the dictionary
save_path = "/ibex/scratch/medinils/breast_data/data/process/graphs.pkl"
with open(save_path, 'rb') as file:
    graphs = pickle.load(file)


# Convert your networkx graphs to PyTorch Geometric data objects
data_list = [from_networkx(graph) for graph in graphs.values()]



# convert node atributes to tensors
for data in data_list:
    for key, item in data:
        if isinstance(item, np.ndarray):
            data[key] = torch.tensor(item)


type(data_list)


# save the list
save_path = "/ibex/scratch/medinils/breast_data/data/process/data_list.pt"
torch.save(data_list, save_path)


# read the list
save_path = "/ibex/scratch/medinils/breast_data/data/process/data_list.pt"
data_list = torch.load(save_path)


# Subset the data
#data_list_subset = data_list[:20]
#patients_subset = list(graphs.keys())[:20]


import psutil
import os

def memory_usage():
    process = psutil.Process(os.getpid())
    return f"Memory usage: {process.memory_info().rss / (1024 ** 3):.2f} GB"



from tqdm.notebook import tqdm

# Dictionary to store embeddings for each graph
graph_embeddings = {}

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

# Loop through each graph in the subset and generate embeddings
for patient, data in tqdm(zip(graphs.keys(), data_list), total=len(data_list), desc="Processing Graphs"):
    
    # Initialize Node2Vec model with num_negative_samples parameter
    model = Node2Vec(data.edge_index, embedding_dim=64, walk_length=15,
                     context_size=10, walks_per_node=100, num_negative_samples=5).to(device)
    loader = model.loader(batch_size=64, shuffle=True, num_workers=4)
    optimizer = torch.optim.Adam(model.parameters(), lr=0.01)

    # Train the model
    model.train()
    for epoch in tqdm(range(100), desc="Training Node2Vec", leave=False):
        total_loss = 0  # To compute average loss over all batches
        for pos_rw, neg_rw in loader:
            optimizer.zero_grad()
            loss = model.loss(pos_rw.to(device), neg_rw.to(device))
            loss.backward()
            optimizer.step()
            total_loss += loss.item()
        avg_loss = total_loss / len(loader)
        print(f"Epoch: {epoch+1}, Loss: {avg_loss:.4f}")

    # Get embeddings for all nodes
    model.eval()
    with torch.no_grad():
        embeddings = model() 

    # Aggregate the embeddings using mean (or any other method you choose)
    aggregated_embedding = embeddings.mean(dim=0).cpu().detach().numpy()

    # Store aggregated embeddings in the dictionary
    graph_embeddings[patient] = aggregated_embedding
    
    # Print memory usage
    print(memory_usage())



# Specify the path for saving
save_path = "/ibex/scratch/medinils/breast_data/data/embeddings/graph_embeddings_20.npy"

# Save the embeddings to the specified path
#np.save(save_path, graph_embeddings)

# Load the embeddings back from the saved file
graph_embeddings = np.load(save_path, allow_pickle=True)


 from sklearn.decomposition import PCA
from sklearn.manifold import TSNE
import matplotlib.pyplot as plt

# Extracting embeddings and labels from the dictionary
embeddings = np.array(list(graph_dict.values()))
patient_labels = list(graph_dict.keys())

# Apply PCA and transform to 2 dimensions
pca = PCA(n_components=2)
embeddings_pca = pca.fit_transform(embeddings)

# Apply t-SNE
embeddings_tsne = TSNE(n_components=2, random_state=0).fit_transform(embeddings)

# Plot PCA results
plt.figure(figsize=(10, 5))
plt.subplot(1, 2, 1)
plt.scatter(embeddings_pca[:, 0], embeddings_pca[:, 1])
plt.title('PCA plot')

# Plot t-SNE results with labels
plt.subplot(1, 2, 2)
plt.scatter(embeddings_tsne[:, 0], embeddings_tsne[:, 1])

# If you have too many points, this might clutter the plot. Adjust as needed.
for i, label in enumerate(patient_labels):
    plt.annotate(label, (embeddings_tsne[i, 0], embeddings_tsne[i, 1]))

plt.title('t-SNE plot')
plt.tight_layout()
plt.show()



import matplotlib.pyplot as plt
from sklearn.manifold import TSNE
from sklearn.preprocessing import LabelEncoder


# Extract the embeddings and labels
embeddings = X_train
labels_patient = patients_train
labels_center = centers_train
labels_target = y_train

# Perform t-SNE
#tsne = TSNE(n_components=2, random_state=42)
#embeddings_tsne = tsne.fit_transform(embeddings)

tsne = TSNE(n_components=2, perplexity=30, n_iter=300)
transformed_data = tsne.fit_transform(embeddings)

def plot_tsne(embeddings, labels, title):
    plt.figure(figsize=(10, 8))
    
    # Convert string labels to numerical labels
    le = LabelEncoder()
    labels_encoded = le.fit_transform(labels)
    
    scatter = plt.scatter(embeddings[:, 0], embeddings[:, 1], c=labels_encoded, cmap='viridis', s=60, alpha=0.7)
    cbar = plt.colorbar(scatter, ticks=range(len(le.classes_)))
    cbar.set_label('Patient Label')
    cbar.set_ticklabels(le.classes_)  # set the original patient labels as tick labels
    
    plt.title(title)
    plt.show()

# Plot t-SNE colored by patient label
# Note: With many patient labels, this plot might not be too informative.
plot_tsne(embeddings_tsne, labels_patient, 't-SNE colored by Patient Label')

# Plot t-SNE colored by center label
plot_tsne(embeddings_tsne, labels_center, 't-SNE colored by Center Label')

# Plot t-SNE colored by target label
plot_tsne(embeddings_tsne, labels_target, 't-SNE colored by Target Label')



from sklearn.metrics import roc_auc_score
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from tqdm.notebook import tqdm
import numpy as np


graph_dict = graph_embeddings.item()
print("Sample keys from graph_dict:", list(graph_dict.keys())[:5])


# Replace the slide-level averaging with graph embeddings
# Filter the patients_train, y_train, and centers_train lists to include only patients 
# for whom you have graph embeddings
filtered_indices = [i for i, patient in enumerate(patients_train) if patient in graph_dict]

X_train = [graph_dict[patients_train[i]] for i in filtered_indices]
y_train = [y_train[i] for i in filtered_indices]
centers_train = [centers_train[i] for i in filtered_indices]
patients_train = [patients_train[i] for i in filtered_indices]

# Convert lists to numpy arrays
X_train = np.array(X_train)
y_train = np.array(y_train)
centers_train = np.array(centers_train)
patients_train = np.array(patients_train)

# Unique patients for stratified K-fold
patients_unique = np.unique(patients_train)



y_train_unique = np.array([np.mean(y_train[patients_train == p]) for p in patients_unique])


from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import StratifiedKFold

# Define the models
models = {
    "Logistic Regression": LogisticRegression(C=0.005, solver="liblinear"),
    "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)
}

# For storing results
results = {model_name: [] for model_name in models}

# Unique patients
patients_unique = np.unique(patients_train)
y_unique = np.array([np.mean(y_train[patients_train == p]) for p in patients_unique])

for k in range(5):
    kfold = StratifiedKFold(5, shuffle=True, random_state=k)
    for train_idx_, val_idx_ in kfold.split(patients_unique, y_unique):
        train_idx = np.where(np.isin(patients_train, patients_unique[train_idx_]))[0]
        val_idx = np.where(np.isin(patients_train, patients_unique[val_idx_]))[0]
        
        X_fold_train, y_fold_train = X_train[train_idx], y_train[train_idx]
        X_fold_val, y_fold_val = X_train[val_idx], y_train[val_idx]
        
        for model_name, model in models.items():
            model.fit(X_fold_train, y_fold_train)
            preds_val = model.predict_proba(X_fold_val)[:, 1]
            auc = roc_auc_score(y_fold_val, preds_val)
            results[model_name].append(auc)
            print(f"{model_name} - Split {k}: AUC: {auc:.3f}")

    print("----------------------------")

for model_name in models:
    print(f"\n{model_name} Results:")
    print(f"AUC: {np.mean(results[model_name]):.3f} ± {np.std(results[model_name]):.3f}")



# Print number of unique patients
print(f"Number of unique patients: {len(np.unique(patients_train))}")

# Print total number of samples
print(f"Total number of samples: {len(X_train)}")



from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import roc_auc_score, accuracy_score, precision_score

# Define the models
models = {
    "Logistic Regression": LogisticRegression(C=0.005, solver="liblinear"),
    "Random Forest": RandomForestClassifier(n_estimators=100, max_depth=5, random_state=0)
}

# 5-fold CV is repeated 5 times with different random states
results = {model_name: {"Train_AUC": [], "Val_AUC": [], "Accuracy": [], "Precision": []} for model_name in models}

patients_unique = np.unique(patients_train)

for k in range(5):
    kfold = StratifiedKFold(5, shuffle=True, random_state=k)
    fold = 0
    # Split is performed at the patient-level
    for train_idx_, val_idx_ in kfold.split(patients_unique, y_train_unique):
        train_idx = np.arange(len(X_train))[np.isin(patients_train, patients_unique[train_idx_])]
        val_idx = np.arange(len(X_train))[np.isin(patients_train, patients_unique[val_idx_])]
        
        # Split data
        X_fold_train, X_fold_val = X_train[train_idx], X_train[val_idx]
        y_fold_train, y_fold_val = y_train[train_idx], y_train[val_idx]
        
        for model_name, model in models.items():
            # Train model
            model.fit(X_fold_train, y_fold_train)
            
            # Predictions on validation data
            preds_val = model.predict_proba(X_fold_val)[:, 1]
            # Predictions on training data
            preds_train = model.predict_proba(X_fold_train)[:, 1]
            
            # Compute metrics for validation data
            val_auc = roc_auc_score(y_fold_val, preds_val)
            acc = accuracy_score(y_fold_val, (preds_val > 0.5).astype(int))
            precision = precision_score(y_fold_val, (preds_val > 0.5).astype(int))
            
            # Compute AUC for training data
            train_auc = roc_auc_score(y_fold_train, preds_train)
            
            # Store results
            results[model_name]["Train_AUC"].append(train_auc)
            results[model_name]["Val_AUC"].append(val_auc)
            results[model_name]["Accuracy"].append(acc)
            results[model_name]["Precision"].append(precision)
            
            print(f"{model_name} - Split {k} Fold {fold}: Train AUC: {train_auc:.3f}, Val AUC: {val_auc:.3f}, Accuracy: {acc:.3f}, Precision: {precision:.3f}")
        
        fold += 1
    print("----------------------------")

# Compute average results
for model_name in models:
    print(f"\n{model_name} Results:")
    print(f"Train AUC: {np.mean(results[model_name]['Train_AUC']):.3f} ± {np.std(results[model_name]['Train_AUC']):.3f}")
    print(f"Val AUC: {np.mean(results[model_name]['Val_AUC']):.3f} ± {np.std(results[model_name]['Val_AUC']):.3f}")
    print(f"Accuracy: {np.mean(results[model_name]['Accuracy']):.3f} ± {np.std(results[model_name]['Accuracy']):.3f}")
    print(f"Precision: {np.mean(results[model_name]['Precision']):.3f} ± {np.std(results[model_name]['Precision']):.3f}")

