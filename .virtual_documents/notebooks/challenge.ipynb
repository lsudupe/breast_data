####packages
from pathlib import Path

import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import pandas as pd
import networkx as nx
import torch
from torch_geometric.data import Data
from scipy.spatial.distance import pdist, squareform
from scipy.spatial import Delaunay
from node2vec import Node2Vec


##select dir
import os
os.getcwd()
data_dir = Path("/ibex/scratch/medinils/breast_data/data/")


##load the data
train_features_dir = data_dir / "train_input" / "moco_features"
test_features_dir = data_dir / "test_input" / "moco_features"
df_train = pd.read_csv(data_dir/"supplementary_data"/"train_metadata.csv")
df_test = pd.read_csv(data_dir/"supplementary_data/test_metadata.csv")

# concatenate y_train and df_train
y_train = pd.read_csv(data_dir  / "train_output.csv")
df_train = df_train.merge(y_train, on="Sample ID")

print(f"Training data dimensions: {df_train.shape}")  # (344, 4)
df_train.head()


#create a graph dic to store them
graphs = {}

X_train = []
y_train = []
centers_train = []
patients_train = []

for sample, patient, center, label in tqdm(
    df_train[["Sample ID", "Patient ID", "Center ID", "Target"]].values
):
    # coordinates and features each iteration one sample
    _features = np.load(train_features_dir / sample)
    # coo and features, separate them
    coo, features = _features[:, 1:3], _features[:, 3:]
    # delaunay triangulation
    tri = Delaunay(coo)
    # Create the graph to add the
    G = nx.Graph()

    # add the edges for each Delaunay triangulation
    for tri in tri.simplices:
        for i in range(3):
            for j in range(i+1, 3):
                # add the edge between the i-th and j-th point in the triangle
                # the edge weight is the euclidean distance
                p1 = coo[tri[i]]
                p2 = coo[tri[j]]
                distance = np.linalg.norm(p1 - p2)
                G.add_edge(tri[i], tri[j], weight =distance)
                # add features as node attributes
                G.nodes[tri[i]].update({'features': features[tri[i]]})
                G.nodes[tri[j]].update({'features': features[tri[j]]})
    # store the graphs for each patient (key)
    graphs[patient] = G
    
    # Slide-level averaging
    X_train.append(np.mean(features, axis=0))
    y_train.append(label)
    centers_train.append(center)
    patients_train.append(patient)


## check the first graph
G1 = next(iter(graphs.values()))
print("Number of nodes (tiles)", len(G1.nodes))

# Calculate the layout with more spacing
pos = nx.spring_layout(G1, k=0.10)
nx.draw_networkx(G1, pos, node_size=10, width= 0.3,with_labels=False)
plt.show()

features = G1.nodes[1]["features"]



import random

# Assuming your graphs are stored in a dictionary called "graphs"
all_graphs = list(graphs.items())  # Convert dictionary to a list of (patient, graph) pairs
sample_size = 50  # Change this to the number of graphs you want in your sample

# Randomly sample graphs
sampled_graphs = dict(random.sample(all_graphs, sample_size))


sampled_graphs


import psutil

def memory_usage():
    process = psutil.Process(os.getpid())
    return f"Memory usage: {process.memory_info().rss / (1024 ** 3):.2f} GB"

# Example usage:
print(memory_usage())


from tqdm.notebook import tqdm
from ipywidgets import FloatProgress


# Dictionary to store embeddings for each graph
graph_embeddings = {}

# Loop through each graph and generate embeddings
for patient, graph in tqdm(sampled_graphs.items(), desc="Generating embeddings"):
    # Initialize and train Node2Vec model for the current graph
    node2vec = Node2Vec(graph, dimensions=64, walk_length=15, num_walks=100, workers=4)
    model = node2vec.fit(window=10, min_count=1, batch_words=4)
    # Store aggregated embeddings in the dictionary
    graph_embeddings[patient] = np.mean([model.wv[str(node)] for node in graph.nodes()])


import json

dir_data = '/ibex/scratch/medinils/breast_data/data/'
file_path = os.path.join(dir_data, 'embeddings/embeddings_50.csv')

# Convert the dictionary to a pandas DataFrame
df = pd.DataFrame.from_dict(graph_embeddings, orient='index')

# Save the DataFrame as a CSV file
df.to_csv(file_path, header=False, index=True)


# Read the CSV file into a pandas DataFrame
df = pd.read_csv(file_path, header=None, index_col=0)

# Convert the DataFrame to a dictionary
embeddings = df.to_dict(orient='index')


embeddings


import numpy as np
from sklearn.model_selection import cross_val_score, StratifiedKFold
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.metrics import classification_report, accuracy_score
from sklearn.preprocessing import StandardScaler
from tensorflow import keras
from tensorflow.keras import layers


emb = np.array(list(embeddings.values()))
labels = np.array(list(embeddings.values()))

# Set up cross-validation
kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

# Evaluate SVM
svm = SVC()
svm_scores = cross_val_score(svm, embeddings, labels, cv=kf, scoring='accuracy')
print(f"SVM Accuracy: {svm_scores.mean():.2f} +/- {svm_scores.std():.2f}")

# Evaluate Decision Trees
dt = DecisionTreeClassifier()
dt_scores = cross_val_score(dt, embeddings, labels, cv=kf, scoring='accuracy')
print(f"Decision Tree Accuracy: {dt_scores.mean():.2f} +/- {dt_scores.std():.2f}")

# Evaluate Neural Network
def create_nn_model():
    model = keras.Sequential([
        layers.Dense(16, activation='relu', input_shape=(embeddings.shape[1],)),
        layers.Dense(8, activation='relu'),
        layers.Dense(1, activation='sigmoid')
    ])
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    return model

nn = keras.wrappers.scikit_learn.KerasClassifier(build_fn=create_nn_model, epochs=30, batch_size=16, verbose=0)
nn_scores = cross_val_score(nn, embeddings, labels, cv=kf, scoring='accuracy')
print(f"Neural Network Accuracy: {nn_scores.mean():.2f} +/- {nn_scores.std():.2f}")



