####packages
from pathlib import Path

import matplotlib.pyplot as plt
from tqdm import tqdm
import numpy as np
import pandas as pd
import networkx as nx
import torch
from torch_geometric.data import Data
from scipy.spatial.distance import pdist, squareform
from scipy.spatial import Delaunay
from node2vec import Node2Vec


##select dir
import os
os.getcwd()
data_dir = Path("/ibex/scratch/medinils/breast_data/data/")


##load the data
train_features_dir = data_dir / "train_input" / "moco_features"
test_features_dir = data_dir / "test_input" / "moco_features"
df_train = pd.read_csv(data_dir/"supplementary_data"/"train_metadata.csv")
df_test = pd.read_csv(data_dir/"supplementary_data/test_metadata.csv")

# concatenate y_train and df_train
y_train = pd.read_csv(data_dir  / "train_output.csv")
df_train = df_train.merge(y_train, on="Sample ID")

print(f"Training data dimensions: {df_train.shape}")  # (344, 4)
df_train.head()


#create a graph dic to store them
graphs = {}

X_train = []
y_train = []
centers_train = []
patients_train = []

for sample, patient, center, label in tqdm(
    df_train[["Sample ID", "Patient ID", "Center ID", "Target"]].values
):
    # coordinates and features each iteration one sample
    _features = np.load(train_features_dir / sample)
    # coo and features, separate them
    coo, features = _features[:, 1:3], _features[:, 3:]
    # delaunay triangulation
    tri = Delaunay(coo)
    # Create the graph to add the
    G = nx.Graph()

    # add the edges for each Delaunay triangulation
    for tri in tri.simplices:
        for i in range(3):
            for j in range(i+1, 3):
                # add the edge between the i-th and j-th point in the triangle
                # the edge weight is the euclidean distance
                p1 = coo[tri[i]]
                p2 = coo[tri[j]]
                distance = np.linalg.norm(p1 - p2)
                G.add_edge(tri[i], tri[j], weight =distance)
                # add features as node attributes
                G.nodes[tri[i]].update({'features': features[tri[i]]})
                G.nodes[tri[j]].update({'features': features[tri[j]]})
    # store the graphs for each patient (key)
    graphs[patient] = G
    
    # Slide-level averaging
    X_train.append(np.mean(features, axis=0))
    y_train.append(label)
    centers_train.append(center)
    patients_train.append(patient)


## check the first graph
G1 = next(iter(graphs.values()))
print("Number of nodes (tiles)", len(G1.nodes))

# Calculate the layout with more spacing
pos = nx.spring_layout(G1, k=0.10)
nx.draw_networkx(G1, pos, node_size=10, width= 0.3,with_labels=False)
plt.show()

features = G1.nodes[1]["features"]



import random

# Assuming your graphs are stored in a dictionary called "graphs"
all_graphs = list(graphs.items())  # Convert dictionary to a list of (patient, graph) pairs
sample_size = 2  # Change this to the number of graphs you want in your sample

# Randomly sample graphs
sampled_graphs = dict(random.sample(all_graphs, sample_size))


import psutil

def memory_usage():
    process = psutil.Process(os.getpid())
    return f"Memory usage: {process.memory_info().rss / (1024 ** 3):.2f} GB"

# Example usage:
print(memory_usage())


from tqdm.notebook import tqdm
from ipywidgets import FloatProgress


# Dictionary to store embeddings for each graph
graph_embeddings = {}

# Loop through each graph and generate embeddings
for patient, graph in tqdm(sampled_graphs.items(), desc="Generating embeddings"):
    # Initialize and train Node2Vec model for the current graph
    node2vec = Node2Vec(graph, dimensions=64, walk_length=15, num_walks=100, workers=4)
    model = node2vec.fit(window=10, min_count=1, batch_words=4)
    # Store aggregated embeddings in the dictionary
    graph_embeddings[patient] = np.mean([model.wv[str(node)] for node in graph.nodes()])
